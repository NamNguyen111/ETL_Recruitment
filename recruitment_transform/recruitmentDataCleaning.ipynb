{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"recruitment_transform\").getOrCreate()\n",
    "\n",
    "# Read all JSON files from the directory into a single DataFrame\n",
    "df = spark.read.json(\"../recruitment_extract\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the corrupt records\n",
    "df.cache() # Push the dataframe to RAM because the spark laziness can cause me an error :)\n",
    "clean_corrupt_df = df.filter(df._corrupt_record.isNull())\n",
    "\n",
    "record_count = clean_corrupt_df.count()\n",
    "\n",
    "# Drop the _corrupt_record column from dataframe\n",
    "clean_corrupt_df = clean_corrupt_df.drop(\"_corrupt_record\")\n",
    "\n",
    "# Show the schema and verify the column is removed\n",
    "clean_corrupt_df.printSchema()\n",
    "# clean_df.show(20)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of records in clean_df: {record_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern for Korean characters\n",
    "korean_pattern = \"[\\uAC00-\\uD7A3]\"  # This range includes Hangul syllables\n",
    "\n",
    "# Remove records with Korean characters in the job_description\n",
    "clean_korean = clean_corrupt_df.filter(~clean_corrupt_df.job_description.rlike(korean_pattern))\n",
    "\n",
    "# Count remaining records\n",
    "remaining_count = clean_korean.count()\n",
    "print(f\"Number of records after removing Korean entries: {remaining_count}\")\n",
    "\n",
    "# Drop the job_schedule since it might be too complex to extract it from raw data column from clean_df\n",
    "clean_df = clean_korean.drop(\"job_schedule\")\n",
    "\n",
    "# Show the schema and verify the column is removed\n",
    "clean_df.printSchema()\n",
    "clean_df.show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, when, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def split_job_description(clean_df):\n",
    "    \"\"\"\n",
    "    I want to split job_description to 2 parts, the real job description and the job requirements\n",
    "    Data from the careerlink website have the key is \"Kinh nghiệm / Kỹ năng chi tiết\" while vietnamworks have a key is \"Yêu cầu công việc\"\n",
    "    \"\"\"\n",
    "    # Define the conditions for splitting the job_description\n",
    "    clean_df_split = clean_df.withColumn(\n",
    "        \"job_description_s\",\n",
    "        when(col(\"job_description\").contains(\"Yêu cầu công việc\"), split(col(\"job_description\"), \"Yêu cầu công việc\").getItem(0))\n",
    "        .when(col(\"job_description\").contains(\"Kinh nghiệm / Kỹ năng chi tiết\"), split(col(\"job_description\"), \"Kinh nghiệm / Kỹ năng chi tiết\").getItem(0))\n",
    "        .otherwise(col(\"job_description\"))\n",
    "    ).withColumn(\n",
    "        \"job_requirements\",\n",
    "        when(col(\"job_description\").contains(\"Yêu cầu công việc\"), concat(lit(\"Yêu cầu công việc\"), split(col(\"job_description\"), \"Yêu cầu công việc\").getItem(1)))\n",
    "        .when(col(\"job_description\").contains(\"Kinh nghiệm / Kỹ năng chi tiết\"), concat(lit(\"Kinh nghiệm / Kỹ năng chi tiết\"), split(col(\"job_description\"), \"Kinh nghiệm / Kỹ năng chi tiết\").getItem(1)))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Select and return the new DataFrame with the new columns\n",
    "    return clean_df_split\n",
    "\n",
    "# Apply the function to your clean_df\n",
    "split_df = split_job_description(clean_df)\n",
    "\n",
    "# Show the result\n",
    "split_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = split_df.drop('job_description') # Drop the old job description since i have it splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the redundant \\n in the data, i keep the \"\\n\" at the end of every line because i might need them later\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def reduce_newlines(df: DataFrame, columns: list) -> DataFrame:\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(column_name, regexp_replace(col(column_name), r\"\\n+\", \"\\n\"))\n",
    "    return df\n",
    "\n",
    "df_cleaned_n_1= reduce_newlines(split_df, [\"job_description_s\", \"job_requirements\"])\n",
    "df_cleaned_n_1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where \"job_links\" contains \"career_link\"\n",
    "filtered_df = df_cleaned_n_1.filter(col(\"job_link\").like(\"%careerlink%\"))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few <br> tag in the data need to be clean\n",
    "def remove_br_tags(df: DataFrame, columns: list) -> DataFrame:\n",
    "    for column_name in columns:\n",
    "        df = df.withColumn(column_name, regexp_replace(col(column_name), \"<br\\\\s*/?>\", \"\"))\n",
    "    return df\n",
    "\n",
    "df_cleaned_br = remove_br_tags(df_cleaned_n_1, [\"job_description_s\", \"job_requirements\"])\n",
    "df_cleaned_br.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "A problem with data from Career link website is that some of their fields contain reduntant \"\\n\", and it \n",
    "need to be clean before calculate the expire date so do i\n",
    "\"\"\"\n",
    "\n",
    "def clean_newlines(df: DataFrame, columns: list) -> DataFrame:\n",
    "    for column in columns:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            F.trim(F.regexp_replace(F.col(column), r'\\n+', ' '))  # Replace multiple newlines with a space\n",
    "        )  # Corrected placement of the closing parenthesis\n",
    "    return df\n",
    "\n",
    "columns_to_clean = [\"job_expire_date\",\"job_location\", \"job_salary\", \"job_title\", \"job_yoe\"]  # Example column names\n",
    "df_cleaned_n_2 = clean_newlines(df_cleaned_br, columns_to_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where \"job_links\" contains \"career_link\"\n",
    "filtered_df = df_cleaned_n_2.filter(col(\"job_link\").like(\"%careerlink%\"))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "# Calculate the exact expire date of the job (format dd/mm/yyyy) base on the column job_expire_date which the data is like \"Het han trong x ngay\"\n",
    "def convert_expiry_date(df: DataFrame, crawl_date: str, date_col: str) -> DataFrame:\n",
    "    # Define the crawl date\n",
    "    crawl_date_col = F.to_date(F.lit(crawl_date), \"dd/MM/yyyy\")\n",
    "    \n",
    "    # Extract the number of days from the \"job_expire_date\" column and calculate the absolute expiration date\n",
    "    df = df.withColumn(\n",
    "        date_col,\n",
    "        F.date_format(\n",
    "            F.date_add(crawl_date_col, F.regexp_extract(F.col(date_col), r\"(\\d+)\", 1).cast(\"int\")), \n",
    "            \"dd/MM/yyyy\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df_update_expire_date = convert_expiry_date(df_cleaned_n_2, \"13/10/2024\", \"job_expire_date\")\n",
    "df_update_expire_date.show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where \"job_links\" contains \"career_link\"\n",
    "filtered_df = df_cleaned_n_2.filter(col(\"job_link\").like(\"%careerlink%\"))\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update_expire_date.write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\", \"../recruitment_load\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update_expire_date.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
