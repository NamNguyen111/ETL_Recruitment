<h1>ETL Pipeline for Job Recruitment Posting (Vietnamese recruitment post crawlers)</h1>
<h3>What this project can do:</h3>
- Extract data from 2 websites vietnamworks.com and careerlink.vn as json files <br>
- Clean and normalize data using PySpark dataframe <br>
- The data can be use to insert to databases or store as data files as your choice <br>
From this <br>

![image](https://github.com/user-attachments/assets/da57072d-479f-4f5d-aafe-7ad5297542f5)

<br>To this <br>

![image](https://github.com/user-attachments/assets/a087b25b-cfa7-4247-9835-cbc53488abe8)
<h3>Requirements:</h3>
- Python 3.10.11 installed
- Scrapy and Scrapy Playwright newest version(use pip install scrapy and pip install scrapy-playwright)
- PySpark 3.5.3 install
- I store data as parquet files so no DBMS required

For now im learning to use Linux and DSA so this project is pause for a time, i will update Docker to this project and move to Linux later.

